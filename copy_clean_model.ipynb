{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 13:57:18.581736: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-14 13:57:18.607622: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-14 13:57:18.608094: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-14 13:57:19.108234: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers, callbacks, activations, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_arrays(data_path):\n",
    "    dataset = h5py.File(data_path, 'r')\n",
    "    x_train = np.array(dataset['X_train']).transpose([0,2,1])\n",
    "    y_train = np.array(dataset['Y_train'])\n",
    "    x_valid = np.array(dataset['X_valid']).transpose([0,2,1])\n",
    "    y_valid = np.array(dataset['Y_valid'])\n",
    "    x_test = np.array(dataset['X_test']).transpose([0,2,1])\n",
    "    y_test = np.array(dataset['Y_test'])\n",
    "    alphabet = 'ACGT'\n",
    "\n",
    "    \n",
    "    return x_train, y_train, x_valid, y_valid, x_test, y_test, alphabet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid, x_test, y_test, alphabet = data_arrays(data_path= '/home/pgarcia/rbp_project/data/HNRNPK_K562_200.h5')\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_shape_check(x_train, x_test,x_valid):\n",
    "    if x_train.shape[-1] == 4:\n",
    "        print(\"Input shape is correct: \" + x_train.shape)\n",
    "    else:\n",
    "        # Adjust input shape\n",
    "        x_train = x_train[:,:,:4]\n",
    "        x_test = x_test[:,:,:4]\n",
    "        x_valid = x_valid[:,:,:4]\n",
    "\n",
    "        # Print the shape of the training data\n",
    "        print(\"Input shape adjusted:\")\n",
    "        print(x_train.shape)\n",
    "        print(x_test.shape)\n",
    "        print(x_valid.shape)\n",
    "    return x_train, x_test, x_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape adjusted:\n",
      "(11328, 200, 4)\n",
      "(3237, 200, 4)\n",
      "(1619, 200, 4)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, x_valid = input_shape_check(x_train, x_test,x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11328, 200, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test#checks whether classification or regression task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepBind Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepbind_test():\n",
    "    #Build the model\n",
    "    model = models.Sequential()\n",
    "    #layer1\n",
    "    model.add(layers.InputLayer(input_shape=(200, 4))) # 4 channel input\n",
    "    #layer2\n",
    "    model.add(layers.Conv1D(filters=16, kernel_size=24, padding='same'))\n",
    "    model.add(layers.Activation(activations.relu))\n",
    "    #layer3\n",
    "    model.add(layers.MaxPooling1D(pool_size=25))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(units=32, activation='relu')) #model says \"one hidden layer with 32 ReLu units\"?\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    \n",
    "    #print(model.summary())\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = deepbind_test()\n",
    "loss = keras.losses.BinaryCrossentropy()\n",
    "opti = tf.keras.optimizers.Adam(learning_rate = 0.005)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=loss,\n",
    "    optimizer=opti, \n",
    "    metrics=[\n",
    "        tf.keras.metrics.AUC(curve=\"ROC\", name=\"auroc\"),\n",
    "        tf.keras.metrics.AUC(curve=\"PR\", name=\"aupr\")  # add AUPR curve to track dataset bias\n",
    "    ]\n",
    "    )\n",
    "\n",
    "# Define an early stopping callback\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    x_train, y_train, \n",
    "    batch_size=100, \n",
    "    epochs=100, \n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks=[es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepBind Exp in first Conv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepbind_exp_test():\n",
    "    #Build the model\n",
    "    model = models.Sequential()\n",
    "    #layer1\n",
    "    model.add(layers.InputLayer(input_shape=(200, 4))) # 4 channel input\n",
    "    #layer2\n",
    "    model.add(layers.Conv1D(filters=16, kernel_size=24, padding='same'))\n",
    "    model.add(layers.Activation(activations.exponential))\n",
    "    #layer3\n",
    "    model.add(layers.MaxPooling1D(pool_size=25))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(units=32, activation='relu')) #model says \"one hidden layer with 32 ReLu units\"?\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    \n",
    "    #print(model.summary())\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = deepbind_exp_test()\n",
    "loss = keras.losses.BinaryCrossentropy()\n",
    "opti = tf.keras.optimizers.Adam(learning_rate = 0.005)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=loss,\n",
    "    optimizer=opti, \n",
    "    metrics=[\n",
    "        tf.keras.metrics.AUC(curve=\"ROC\", name=\"auroc\"),\n",
    "        tf.keras.metrics.AUC(curve=\"PR\", name=\"aupr\")  # add AUPR curve to track dataset bias\n",
    "    ]\n",
    "    )\n",
    "\n",
    "# Define an early stopping callback\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    x_train, y_train, \n",
    "    batch_size=100, \n",
    "    epochs=100, \n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks=[es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saliency Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get model predictions for test sequences\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Get the top num_plots predictions\n",
    "num_plots = 10\n",
    "\n",
    "# Get the sorted indices\n",
    "sorted_indices = np.argsort(predictions[:, 0])[::-1]\n",
    "\n",
    "# Extract the top num_plots sequences\n",
    "X = x_test[sorted_indices[:num_plots]]\n",
    "\n",
    "# Reshape X to (num_plots, 200, 4)\n",
    "X = X.reshape((num_plots, 200, 4))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################\n",
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def calculate_saliency_map(X, model, class_index=0):\n",
    "  \"\"\"fast function to generate saliency maps\"\"\"\n",
    "  if not tf.is_tensor(X):\n",
    "    X = tf.Variable(X)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    tape.watch(X)\n",
    "    output = model(X)[:,class_index]\n",
    "  return tape.gradient(output, X)\n",
    "\n",
    "saliency_map = calculate_saliency_map(X, model)\n",
    "saliency_map = saliency_map.numpy()\n",
    "\n",
    "#########################################################\n",
    "import pandas as pd\n",
    "import logomaker\n",
    "\n",
    "def plot_saliency_map(scores, alphabet, ax=None):\n",
    "  L,A = scores.shape\n",
    "  counts_df = pd.DataFrame(data=0.0, columns=list(alphabet), index=list(range(L)))\n",
    "  for a in range(A):\n",
    "    for l in range(L):\n",
    "      counts_df.iloc[l,a] = scores[l,a]\n",
    "\n",
    "  if not ax:\n",
    "    ax = plt.subplot(1,1,1)\n",
    "  logomaker.Logo(counts_df, ax=ax)\n",
    "\n",
    "\n",
    "saliency_scores = saliency_map * X\n",
    "for scores in saliency_scores:\n",
    "  fig = plt.figure(figsize=(20,1))\n",
    "  ax = plt.subplot(1,1,1)\n",
    "  plot_saliency_map(scores, alphabet, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('/home/pgarcia/rbp_project/models_deepbind')# Creating a directory\n",
    "# model.save('models_deepbind/HNRNPK_K562_200.h5')   # Saving model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repre_test():\n",
    "    \n",
    "    #Build the model\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = models.Sequential()\n",
    "    # layer1\n",
    "    model.add(layers.InputLayer(input_shape=(200, 4))) # 4 channel input\n",
    "    \n",
    "    # layer2\n",
    "    l2_regularizer = tf.keras.regularizers.L2(1e-6)\n",
    "    model.add(layers.Conv1D(filters=30, kernel_size=19, strides=1, padding='same', kernel_regularizer=l2_regularizer))\n",
    "    # add batch normalization\n",
    "    # batch_norm = tf.keras.layers.BatchNormalization()\n",
    "    model.add(layers.Activation(activations.relu))\n",
    "\n",
    "    # layer3\n",
    "    model.add(layers.MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    \n",
    "    model.add(layers.Conv1D(filters=30, kernel_size=19, strides=1, padding='same'))\n",
    "    # model.add(layers.Conv1D(filters=30, kernel_size=19, strides= 1, padding='same', kernel_regularizer=l2_regularizer))\n",
    "    model.add(layers.MaxPooling1D(pool_size=50, strides=50))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    model.add(layers.Dense(units=512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    # model.summary()\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model = repre_test()\n",
    "loss = keras.losses.BinaryCrossentropy()\n",
    "opti = tf.keras.optimizers.Adam(learning_rate=0.003)\n",
    "model.compile(\n",
    "    loss=loss, \n",
    "    optimizer=opti, \n",
    "    metrics=[\n",
    "        tf.keras.metrics.AUC(curve=\"ROC\", name=\"auroc\"),\n",
    "        tf.keras.metrics.AUC(curve=\"PR\", name=\"aupr\")\n",
    "    ]\n",
    "    ) # add AUPR curve to track dataset bias\n",
    "\n",
    "# Define an early stopping callback\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train, \n",
    "    batch_size=100, \n",
    "    epochs=100, \n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks=[es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation Learning Model w/ Exp function in 1D Conv Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repre_exp_test():\n",
    "    \n",
    "    #Build the model\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = models.Sequential()\n",
    "    # layer1\n",
    "    model.add(layers.InputLayer(input_shape=(200, 4))) # 4 channel input\n",
    "    \n",
    "    # layer2\n",
    "    l2_regularizer = tf.keras.regularizers.L2(1e-6)\n",
    "    model.add(layers.Conv1D(filters=30, kernel_size=19, strides=1, padding='same', kernel_regularizer=l2_regularizer))\n",
    "    # add batch normalization\n",
    "    # batch_norm = tf.keras.layers.BatchNormalization()\n",
    "    model.add(layers.Activation(activations.relu))\n",
    "\n",
    "    # layer3\n",
    "    model.add(layers.MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    \n",
    "    model.add(layers.Conv1D(filters=30, kernel_size=19, strides=1, padding='same'))\n",
    "    # model.add(layers.Conv1D(filters=30, kernel_size=19, strides= 1, padding='same', kernel_regularizer=l2_regularizer))\n",
    "    model.add(layers.MaxPooling1D(pool_size=50, strides=50))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    model.add(layers.Dense(units=512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    # model.summary()\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 13:57:41.299747: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 2s 8ms/step - loss: 0.2907 - auroc: 0.9414 - aupr: 0.9537 - val_loss: 0.2203 - val_auroc: 0.9603 - val_aupr: 0.9720\n",
      "Epoch 2/100\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.2017 - auroc: 0.9691 - aupr: 0.9748 - val_loss: 0.2643 - val_auroc: 0.9671 - val_aupr: 0.9768\n",
      "Epoch 3/100\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.1883 - auroc: 0.9733 - aupr: 0.9783 - val_loss: 0.2172 - val_auroc: 0.9704 - val_aupr: 0.9779\n",
      "Epoch 4/100\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.1722 - auroc: 0.9782 - aupr: 0.9820 - val_loss: 0.2034 - val_auroc: 0.9707 - val_aupr: 0.9775\n",
      "Epoch 5/100\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.1508 - auroc: 0.9838 - aupr: 0.9859 - val_loss: 0.1886 - val_auroc: 0.9733 - val_aupr: 0.9788\n",
      "Epoch 6/100\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.1388 - auroc: 0.9864 - aupr: 0.9883 - val_loss: 0.2400 - val_auroc: 0.9700 - val_aupr: 0.9773\n",
      "Epoch 7/100\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.1319 - auroc: 0.9881 - aupr: 0.9889 - val_loss: 0.2279 - val_auroc: 0.9697 - val_aupr: 0.9768\n",
      "Epoch 8/100\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.1188 - auroc: 0.9903 - aupr: 0.9911 - val_loss: 0.2314 - val_auroc: 0.9690 - val_aupr: 0.9762\n",
      "Epoch 9/100\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0974 - auroc: 0.9935 - aupr: 0.9941 - val_loss: 0.2720 - val_auroc: 0.9666 - val_aupr: 0.9681\n",
      "Epoch 10/100\n",
      "114/114 [==============================] - 1s 6ms/step - loss: 0.0907 - auroc: 0.9943 - aupr: 0.9946 - val_loss: 0.2247 - val_auroc: 0.9702 - val_aupr: 0.9767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f9588c84af0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compile the model\n",
    "model = repre_exp_test()\n",
    "loss = keras.losses.BinaryCrossentropy()\n",
    "opti = tf.keras.optimizers.Adam(learning_rate=0.003)\n",
    "model.compile(\n",
    "    loss=loss, \n",
    "    optimizer=opti, \n",
    "    metrics=[\n",
    "        tf.keras.metrics.AUC(curve=\"ROC\", name=\"auroc\"),\n",
    "        tf.keras.metrics.AUC(curve=\"PR\", name=\"aupr\")\n",
    "    ]\n",
    "    ) # add AUPR curve to track dataset bias\n",
    "\n",
    "# Define an early stopping callback\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train, \n",
    "    batch_size=100, \n",
    "    epochs=100, \n",
    "    validation_data=(x_valid, y_valid),\n",
    "    callbacks=[es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 1ms/step - loss: 0.1849 - auroc: 0.9781 - aupr: 0.9817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18492381274700165, 0.9780533909797668, 0.9816553592681885]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate the model\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saliency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
